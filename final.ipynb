{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuabaron/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuabaron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk library\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re   # Regular expressions\n",
    "import requests\n",
    "from io import StringIO\n",
    "import string\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import( \n",
    "    roc_auc_score, \n",
    "    roc_curve,\n",
    "    auc, \n",
    "    confusion_matrix,\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.preprocessing import minmax_scale, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Misc\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "from scipy.sparse import hstack\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS = {\n",
    "    \"df_9\": 'https://drive.google.com/file/d/1PgYUKf5awbdVF_9iHvz8AynBEfTNrWXK/view?usp=sharing',\n",
    "    \"df_20\": 'https://drive.google.com/file/d/1DNvxeXPbknDCjShonzJwgLVIvgC1A8Ae/view?usp=sharing'\n",
    "}\n",
    "def read_from_google_sheet(url: str) -> pd.DataFrame:\n",
    "    file_id = url.split('/')[-2]\n",
    "    dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    url = requests.get(dwn_url).text\n",
    "    csv_raw = StringIO(url)\n",
    "    return pd.read_csv(csv_raw)\n",
    "\n",
    "def read_data():\n",
    "    df = read_from_google_sheet(LINKS['df_9'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_data()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-format Data\n",
    "- Offensive tweets = all tweets labeled as 0 or 1 -> Re-label as 0\n",
    "- Non-offensive = all tweets labeled as 2 -> Re-label as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of offensive samples: 20620\n",
      "Number of non-offensive samples: 4163\n"
     ]
    }
   ],
   "source": [
    "df = df[['tweet', 'class']]\n",
    "df['class'] = df['class'].apply(lambda x: 1 if x == 2 else 0)\n",
    "print('Number of offensive samples:', df[df['class'] == 0].shape[0])\n",
    "print('Number of non-offensive samples:', df[df['class'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet parsing\n",
    "def preprocess(tweet: str):\n",
    "    space_pattern = '\\s+'\n",
    "    url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[^\\s]+'\n",
    "    symbol_regex = '&#[^\\s]+'\n",
    "    \n",
    "    parsed_tweet = tweet.lower()\n",
    "    parsed_tweet = re.sub(space_pattern, ' ', parsed_tweet)\n",
    "    parsed_tweet = re.sub(url_regex, 'URLHERE', parsed_tweet)\n",
    "    parsed_tweet = re.sub(symbol_regex, ' ', parsed_tweet)\n",
    "    parsed_tweet = re.sub(mention_regex, 'MENTIONHERE', parsed_tweet)\n",
    "    \n",
    "    return parsed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(doc: str):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(doc)\n",
    "    filtered_words = [word for word in words if not word in all_stopwords and word.isalnum()]\n",
    "    \n",
    "    return [stemmer.stem(word) for word in filtered_words if word not in ['URLHERE', 'MENTIONHERE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords += ['user', '@', '!', 'rt', 'http', 'lol', 'like', 'amp', 'co', 'get', 'ff']\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=preprocess,\n",
    "    stop_words=all_stopwords,\n",
    "    min_df=5,\n",
    "    max_df=.75,\n",
    "    analyzer=analyzer,\n",
    "    ngram_range=(2,4),\n",
    "    smooth_idf=False,\n",
    "    max_features=10000\n",
    ")\n",
    "# X_tweet = vectorizer.fit_transform(df.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    stop_words=all_stopwords,\n",
    "    min_df=5,\n",
    "    max_df=.5,\n",
    "    ngram_range=(3,5),\n",
    "    smooth_idf=False,\n",
    "    max_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(df):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = defaultdict(list)\n",
    "    for i in range(len(df)):\n",
    "        score_dict = sentiment_analyzer.polarity_scores(df[i])\n",
    "        scores['neg'].append(score_dict['neg'])\n",
    "        scores['neu'].append(score_dict['neu'])\n",
    "        scores['pos'].append(score_dict['pos'])\n",
    "        scores['compound'].append(score_dict['compound'])\n",
    "    return np.array(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([\n",
    "    ('word_vec', vectorizer),\n",
    "    ('char_vec', char_vectorizer),\n",
    "    ('sentiment', FunctionTransformer(getSentiment))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28868, 4250) (28868,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_union',\n",
       "                 FeatureUnion(transformer_list=[('word_vec',\n",
       "                                                 TfidfVectorizer(analyzer=<function analyzer at 0x1a23354b90>,\n",
       "                                                                 max_df=0.75,\n",
       "                                                                 max_features=10000,\n",
       "                                                                 min_df=5,\n",
       "                                                                 ngram_range=(2,\n",
       "                                                                              4),\n",
       "                                                                 preprocessor=<function preprocess at 0x1a24a17950>,\n",
       "                                                                 smooth_idf=False,\n",
       "                                                                 stop_words=['i',\n",
       "                                                                             'me',\n",
       "                                                                             'my',\n",
       "                                                                             'myself',\n",
       "                                                                             'we',\n",
       "                                                                             'our',\n",
       "                                                                             'ours',\n",
       "                                                                             'ourselves',\n",
       "                                                                             'you',\n",
       "                                                                             \"you're\",\n",
       "                                                                             \"you've\",\n",
       "                                                                             \"you...\n",
       "                                                                             \"she's\",\n",
       "                                                                             'her',\n",
       "                                                                             'hers',\n",
       "                                                                             'herself',\n",
       "                                                                             'it',\n",
       "                                                                             \"it's\",\n",
       "                                                                             'its',\n",
       "                                                                             'itself', ...],\n",
       "                                                                 tokenizer=<function tokenize at 0x1a22b5e0e0>)),\n",
       "                                                ('sentiment',\n",
       "                                                 FunctionTransformer(func=<function getSentiment at 0x1a2721e8c0>))])),\n",
       "                ('select_from_model',\n",
       "                 SelectFromModel(estimator=LogisticRegression(max_iter=100000))),\n",
       "                ('classifier',\n",
       "                 LinearSVC(class_weight={0: 1, 1: 4.5}, loss='hinge',\n",
       "                           max_iter=1000000))])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = vectorizer.fit_transform(df['tweet']), df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,  test_size=0.3)\n",
    "\n",
    "### SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "print(X_train_sm.shape, y_train_sm.shape)\n",
    "\n",
    "### SMOTE\n",
    "\n",
    "clf = LinearSVC(max_iter=1000000, class_weight={1: 4.5, 0: 1}, loss='hinge')\n",
    "clf.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feature_union', feature_union),\n",
    "    ('select_from_model', SelectFromModel(LogisticRegression(max_iter=100000))),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "pipeline.fit(df['tweet'], y)\n",
    "\n",
    "dump(pipeline, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96      6186\n",
      "           1       0.75      0.94      0.83      1249\n",
      "\n",
      "    accuracy                           0.94      7435\n",
      "   macro avg       0.87      0.94      0.90      7435\n",
      "weighted avg       0.95      0.94      0.94      7435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, clf.predict(X_test))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEGCAYAAAAE8QIHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdUElEQVR4nO3deZhU1Z3/8fe3F/ZFsUE2RVREHRfc10TcADMZNQkmLpnEGZVxzUQnOm5xohPRMb9fMlHJT3F3Ztyjo3FUdFAm4oCIC6ggjRuIgLIvzdJN1ff3x73dVjdN1b10VVfV7c/ree7zVNW9nHOKevhyzj33nK+5OyIiSVFR7AaIiOSTgpqIJIqCmogkioKaiCSKgpqIJEpVsRuQqaZPpe+2S3WxmyEx1M7uVuwmSAybqKPeN1tbyhh9fHdfsTIV6dq3Z2+e5O5j2lJfXCUV1HbbpZoZk3YpdjMkhtEDRxS7CRLDmz65zWWsWJlixqRdI11bOWB+TZsrjKmkgpqIlD4H0qSL3YxtUlATkVgcp8GjDT+LQUFNRGJTT01EEsNxUiW8vFJBTURiS1O6QU3PqYlILA6k8EhHLmY2xszmmdnHZnZ1K+eHmNlkM5ttZlPMbHCuMhXURCS2NB7pyMbMKoEJwCnAvsBZZrZvi8v+D/Cwux8A3ATckqttCmoiEosDDe6RjhwOBz5290/dvR54DDitxTX7Ao0P173WyvmtKKiJSCwecegZDj9rzGxmxjEuo6hBwBcZ7xeFn2WaBfwgfP09oKeZ7ZStfZooEJF4HFLR5wmWu/uh2zjX2nKtliX/ArjTzM4F/gx8CWzJVqGCmojEEqwoyItFQOa6yMHA4mZ1uS8Gvg9gZj2AH7j7mmyFavgpIjEZqYhHDm8Bw8xsqJl1As4EnmtWk1mNmTXGqWuA+3MVqqAmIrEEEwUW6chajvsW4FJgEjAXeMLdPzSzm8zs1PCykcA8M6sFdgZuztU+DT9FJJbgObU27V70TVnuLwAvtPjshozXTwFPxSlTQU1EYkvn6IUVk4KaiMSSz55aISioiUgsjpEq4dvxCmoiEpuGnyKSGI5R75XFbsY2KaiJSCzBw7cafopIgmiiQEQSw91IuXpqIpIgafXURCQpgomC0g0dpdsyESlJmigQkcRJ6Tk1EUkKrSgQkcRJa/ZTRJIiWNCuoCYiCeEYDSW8TKp0w62IlCR3SHlFpCOXCMmMdzWz18zs3TCh8XdylamgJiIxGemIR9ZSoiUzvp5gm++DCHIY/CFX6zT8FJFYHPK1TKopmTGAmTUmM57Torpe4evetMg21RoFNRGJLcZEQY2Zzcx4P9HdJ4avW0tmfESLP/8r4GUzuwzoDpyUq0IFNRGJxbE4m0S2NZnxWcCD7v5/zewo4N/MbD9332bqUQU1EYklSJGXl9CRM5kxcB4wBsDdp5lZF6AG+HpbhWqiQERiar9kxsBC4EQAM9sH6AIsy1aoemoiEouTnxUF7r7FzBqTGVcC9zcmMwZmuvtzwD8A95jZ5WHV57p7yyFqMwpqIhJbOyYzngMcE6dMBTURicXdtPZTRJIjmCgo3WVSCmoiEpNyFIhIggQTBdokUkQSRFsPiUhixFxR0O4U1EQkNiVeEZHEcIeGtIKaiCREMPxUUBORBMnXioJCKN1wWwbeeq0n5x27N+cevQ+P39Fvq/NfLarmH3+4BxeeOJwrf7AnyxZXNztft66Csw/elzuvHdReTe6QDh25lntf/4gH3pjLDy/9aqvz1Z3SXHvX5zzwxlx+//x8dh5c33Ru6D4b+d1z85n42kfcNXke1Z2b73jzqwc/4+5X5xX8O5SSxkc6ohzFUNCglmv/8XKWSsGEawfz6//4lHumfMRrz+7IgtrOza6556ZBnDR2JXdNnsc5ly/lgVsGNDv/8G0D2P/IuvZsdodTUeFcMv5Lrj9nKBeMHM7xp61m12Gbml0z+qyVrF9dxd8csw9P31PDedcHu99UVDpX3bGQO64ezLjj9+bKsXuQavjmH+oxp6xmU11H7BcEw88oRzEUrNaI+4+XrXnvdmPgbpsZMKSe6k7OyNNWMW1S72bXLKjtzIhj1wNw4DHrm52fP7srq5ZVcchx69q13R3N8IM2sPjzTixd2JktDRVMeXYHjhq9ptk1R41ewytP7gjA68/vEP5mziHHreOzuV34dE5XANatqiKdDoJal24pvv93y3jkX3du1+9TKvKRo6BQChlKm/Yfd/d6oHH/8URYsbSavgMbmt7XDGhg+ZLmw8vd993E1BeCQPbGi73ZsL6StSsrSadh4o2DOP+XObdblzbaqX8DyxZ3anq/fEk1NQMaml1T039L062BdMqoW1tJrz4pBu++GXfj5kc+4c5JtZxx8Tf7Ev70qqX88a5+bN7Y8XpqwexnZaSjGAr5i7S2//hWN4/MbJyZzTSzmctWpArYnPxqbUcna/Ef07gbvuT9aT24+OS9eH9aD2oG1FNZ5fzpwRoOO2Et/QY1bF2I5FXL3wS2/u3Mtv4x3aGyytnv8Dr+5dIh/MPpe3L0mDWMOHYdu//FRgYOred/X+q91Z/rCBofvi3Ve2qFnP2Msv84YRKGiQCHHtgl6+ZvpaRmQEOzG//Ll1SzU//mQWqn/lu44b7PAdhYV8HUF3rTvVeauW9344M3e/D8QzVsrKtgS4PRtXua865b0p5foUNYvqSavgO/ufFfM6CBFUub96iXLQl63cuXdKKi0uneK8W6VZUsW1LN7GndWbsy+Gfy1qu92HP/jWyqq2DY/ht46M05VFbCDjVbuO2pj7lq7J7t+t2KKV9DSzMbA/yeYJPIe9391hbnfwccH77tBvRz9x2ylVnIoBZl//GyNXzEBr78rDNLF3Zip/4NTHl2R66esKDZNWtWVNJzxxQVFfDYHf0Y9aOVAFw9YWHTNS8/3ofaWV0V0Apk3nvdGDS0np132cyKpdWMPG01t14ypNk101/uzclnrGLu29351ndXM2tqD8B4e0pPzrh4GZ27pmmoNw44aj1PT+zLjMm9eP7hGgB2HlzPTQ9/1qECWr4WtGfcdz+ZIF68ZWbPhRtDBnW5X55x/WXAQbnKLWRQa9p/HPiSYP/xswtYX7uqrIJLbl7EtWfvTjpljDpzJbsN38RDt/VnrwM3cNTotcye1oP7bxmImbP/EXVcMn5RsZvd4aRTxoTrBjH+kU+pqISXH+vDgtou/OTKpdTO6sr0l3vz0qN9uOr2hTzwxlzWra5k/EVB0Fu/poqn7+7LHS/U4m7MeLUnMyb3ylFjx5Cnmc0oeT8znQX8U65CLcd2320Spoj/V77Zf/zmbNcfemAXnzFpl2yXSIkZPXBEsZsgMbzpk1nrK9vUzdpx735+wv1jI1379DH/bwGwPOOjpryfZjYWGOPu54fv/xo4wt0vbVmOmQ0BpgOD3T3rzfeCrihobf9xESl/7Zj3s9GZwFO5AhpomZSIxJTHTSLj3Hc/E7gkSqEKaiISW56CWqT77mY2HNgRmBalUAU1EYklX5tERsz7CcEEwWO58n02UlATkdjy9Zxarryf4ftfxSlTQU1EYnGHLdokUkSSRDkKRCQxlHhFRBLHFdREJEmKtVdaFApqIhKLu+6piUiiGCnNfopIkuiemogkRh7XfhaEgpqIxOOtb2dfKhTURCQ2zX6KSGK4JgpEJGk0/BSRRNHsp4gkhruCmogkTCk/0lG6d/tEpGS5RztyMbMxZjbPzD42s6u3cc0PzWyOmX1oZo/kKlM9NRGJxTHSeZj9jJLM2MyGAdcAx7j7KjPrl6tc9dREJDaPeOTQlMzY3euBxmTGmS4AJrj7KgB3/zpXoQpqIhJPOFEQ5QBqzGxmxjEuo6RBwBcZ7xeFn2XaC9jLzN4ws+lmNiZX8zT8FJH4oj+n1tZkxlXAMGAkQV7Q181sP3dfva0KtxnUzKxXtpa6+9ps50UkufL0SEeUZMaLgOnu3gB8ZmbzCILcW9sqNFtP7UOCqJnZ+sb3DuwauekikhgOpNPtlsz4Pwnyfj5oZjUEw9FPsxW6zaDm7rts65yIdGAOtF8y40nAKDObA6SAK919RbZyI91TM7Mzgd3dfbyZDQZ2dve32/KFRKR85WvtZ65kxmFW9ivCI5Kcs59mdidwPPDX4UcbgLuiViAiCZSnZzoKIUpP7Wh3P9jM3gVw95Vm1qnA7RKRkmVlv/azwcwqCOOume0EpAvaKhEpbWW+9dAE4I9AXzO7EfghcGNBWyUipcvB8zP7WRA5g5q7P2xmbwMnhR+d4e4fFLZZIlLayjiohSqBBoJOp5ZWiXR0JTz8jDL7eR3wKDCQ4InfR8zsmkI3TERKWJnPfv4YOMTdNwCY2c3A28AthWyYiJSoPD18WyhRgtqCFtdVkWOZgogkW1kmXjGz3xHE5A3Ah2Y2KXw/CpjaPs0TkZJUprOfjTOcHwL/lfH59MI1R0TKgZVjT83d72vPhohImSjiJEAUOe+pmdkewM3AvkCXxs/dfa8CtktESpaV9ERBlGfOHgQeIHja7hTgCYK9xEWkoyrhRzqiBLVu7j4JwN0/cffrCXbtEJGOKh3xKIIoj3RsNjMDPjGzCwl2qMyZpkpEEqrEn1OL0lO7HOgB/Aw4hiBl1d8WslEiUtrMox05y8mRzNjMzjWzZWb2Xnicn6vMKAva3wxfruObjSJFpCPLw/2yKMmMQ4+7+6VRy8328O0zZGm6u38/aiUiIq1oSmYMYGaNyYxbBrVYsvXU7mxLwdujdnY3Rg8c0d7VShtMWvxesZsgMRw+ekNeyonx8G2Nmc3MeD/R3SeGr1tLZnxEK2X8wMy+DdQCl7v7F61c0yTbw7eTo7VZRDoUJ84yqbYmM/4T8Ki7bw4nKh8CTshWofZGE5H48vOcWs5kxu6+wt03h2/vAQ7JVaiCmojElqfZz6ZkxmEypzOB55rVYzYg4+2pwNxchUbd+RYz65wRMUWkI8vD7GfEZMY/M7NTgS3ASuDcXOVGWft5OHAf0BvY1cwOBM5398u2+9uISHlrv2TG1wCxdtqOMvy8HfgusCKsZBZaJiXSYUUdehZre6Iow88Kd18QrJRqkipQe0SkHJTpJpGNvgiHoB4+AXwZwfMiItJBleUmkRkuIhiC7gp8Bfx3+JmIdFTlHNTc/WuCqVYRESji/bIoosx+3kMrcdndxxWkRSJS+so5qBEMNxt1Ab5H8/VaItLBWJE2gIwiyvDz8cz3ZvZvwCsFa5GISBtEXlGQYSgwJN8NEZEyUs7DTzNbxTdfoYJgqcJWO1SKSAdRzhMFYW6CAwnyEgCk3Us54byItIsSjgJZl0mFAewZd0+FRwl/FRFpN2WeIm+GmR1c8JaISFkwgtnPKEcxZMtRUOXuW4BjgQvM7BOgjuA7ubsr0Il0RGV8T20GcDBweju1RUTKRZkGNYMgK3s7tUVEykWZBrW+ZnbFtk66+28L0B4RKQP5Gn6a2Rjg9wQ7397r7rdu47qxwJPAYe4+s7VrGmULapUEmdlLd+MkESmOdkxmbGY9gZ8Bb25dytayBbUl7n7TdrZXRJLK8zazGTWZ8T8DtwG/iFJotkc61EMTkdZFf06txsxmZhyZu/u0lsx4UGY1ZnYQsIu7Px+1adl6aidGLUREOpYY99S2O5mxmVUAvyNCBqlM2+ypufvKOAWJSAfSPsmMewL7AVPM7HPgSOA5M9tWkAS2b5cOEenI8rcEqimZMcH68jOBs5uqcV8D1DS+N7MpwC9yzX4qQ7uIxGLkJ0VeuGKpMZnxXOCJxmTGYQLj7aKemojElq/n1HIlM27x+cgoZSqoiUh8ZbqiQESkdQpqIpIYZbxLh4hI6xTURCRJyjpFnohISxp+ikhyFDH/QBQKaiISn4KaiCRF44qCUqWgJiKxWbp0o5qCmojEo3tqIpI0Gn6KSLIoqIlIkqinJiLJoqAmIomRv2xSBaGdb0UklnztfAtBMmMzm2dmH5vZ1a2cv9DM3jez98xsqpntm6tMBTURic892pFFRjLjU4B9gbNaCVqPuPv+7j6CIPfnb3M1TUFNRGLLU0+tKZmxu9cDjcmMm7j72oy33YlwN0/31GI6dORaLvznxVRWOC8+2ocn7ty52fnqTmmuvH0hw/bfyNpVVYy/cAhfLeoEwNB9NvKzf1lE954p0mnjsu8Mo6ICrrv7cwbuVk86BdNf6cX94wcW46t1CG+91pO7fjmIVNo45awV/Oiyr5ud/2pRNb+9YlfWrKii5w4prrpjAX0HNjSdr1tXwQXH7c3RY9Zw6fgv27v5pSHew7c1ZpaZ/Wmiu08MX7eWzPiIlgWY2SXAFUAn4IRcFRasp2Zm95vZ12b2QaHqaG8VFc4l47/k+nOGcsHI4Rx/2mp2Hbap2TWjz1rJ+tVV/M0x+/D0PTWcd32QxrCi0rnqjoXccfVgxh2/N1eO3YNUQ5DL9Y939eP8b+/NxaP24i8O28Chx6/dqm5pu1QKJlw7mF//x6fcM+UjXnt2RxbUdm52zT03DeKksSu5a/I8zrl8KQ/cMqDZ+YdvG8D+R9a1Z7NLkqWjHYTJjDOOiZnFtFL0VuHS3Se4+x7APwLX52pbIYefDwJjClh+uxt+0AYWf96JpQs7s6WhginP7sBRo9c0u+ao0Wt45ckdAXj9+R0Ycex6wDnkuHV8NrcLn87pCsC6VVWk08bmjRXM+t8eAGxpqGD++13pO6AByb9573Zj4G6bGTCknupOzsjTVjFtUu9m1yyo7Rz+ZnDgMeubnZ8/uyurllVxyHHr2rXdpShGUMsmVzLjlh4DTs9VaMGCmrv/GUhUlved+jewbHGnpvfLl1RT0yIA1fTfwrLF1QCkU0bd2kp69UkxePfNuBs3P/IJd06q5YyLmw97ALr3SnHkyWt5d2qPwn6RDmrF0upmQ8maAQ0sX1Ld7Jrd993E1BeCQPbGi73ZsL6StSsrSadh4o2DOP+X2f7NdRBOXiYKyEhmbGadCJIZP5d5gZkNy3j7l8D8XIUW/Z6amY0DxgF0oVuRW5OdtdJZbvm7WSt3R92hssrZ7/A6LvvOMDZvrODWxz9h/uyuvDe1JxAMT6/5wwKeva+GpQs7b1WGtF1r/8Za/qbjbviSCdcN5pXH+7D/kXXUDKinssr504M1HHbCWvoNUi8a8rOiwN23mFljMuNK4P7GZMbATHd/DrjUzE4CGoBVwE9zlVv0oBaOsScC9LI+JfycctAz6zuwvul9zYAGVixt/j/9siVBb2D5kk5UVDrde6VYt6qSZUuqmT2tO2tXBn/lb73aiz3339gU1H7+my/48rPOPHNv3/b7Qh1MzYCGpl40BL/nTv2bB6md+m/hhvs+B2BjXQVTX+hN915p5r7djQ/e7MHzD9Wwsa6CLQ1G1+5pzrtuSXt+hdLRTsmM3f3v45apRzpimPdeNwYNrWfnXTZTVZ1m5Gmrmf5y83sy01/uzclnrALgW99dzaypPQDj7Sk9GbrvJjp3TVNR6Rxw1HoW1nYB4KdXLaF7zzR33aBZz0IaPmIDX37WmaULO9FQb0x5dkeOHNV8UmbNimCoCfDYHf0Y9aPgDsrVExby7zPn8PCMOVxww2JOHLuywwa0fD58WwhF76mVk3TKmHDdIMY/8ikVlfDyY31YUNuFn1y5lNpZXZn+cm9eerQPV92+kAfemMu61ZWMv2gIAOvXVPH03X2544Va3I0Zr/ZkxuRe1Ayo5+yff83C+Z2Z8HItAM89UMNLj+xUzK+aSJVVcMnNi7j27N1Jp4xRZ65kt+GbeOi2/ux14AaOGr2W2dN6cP8tAzFz9j+ijkvGLyp2s0uPe0lvEmme+2be9hVs9igwEqgBvgL+yd3vy/ZnelkfP8JOLEh7pDAmLX6v2E2QGA4f/QUzZ21q7VGKyHruMNgP+na0UeHrf7rqbXc/tC31xVWwnpq7n1WoskWkuLT1kIgkhwMlPPxUUBOR+Eo3pimoiUh8Gn6KSKKU8uyngpqIxKMUeSKSJMHDt6Ub1RTURCS+Es5RoKAmIrGppyYiyaF7aiKSLKW99lNBTUTiK+Hhp7YeEpF4PG/beUfJ+3mFmc0xs9lmNtnMhuQqU0FNROJrv7yf7wKHuvsBwFMEuT+zUlATkfg84pFdlLyfr7n7hvDtdILkLFnpnpqIxGbpyA+qtTnvZ4bzgBdzVaigJiLxOHEevl2eZZPISHk/Aczsx8ChwHG5KlRQE5FYDM/Xw7eR8n6G2aSuA45z9825CtU9NRGJr/3yfh4E3A2c6u5bJ8tthXpqIhJfHnpqEfN+/gboATxpQZLWhe5+arZyFdREJJ5499SyF5U77+dJcctUUBOR2GLMfrY7BTURiSnS/bKiUVATkXgcBTURSZjSHX0qqIlIfNokUkSSRUFNRBLDHVKlO/5UUBOR+NRTE5FEUVATkcRwQDkKRCQ5HFz31EQkKRxNFIhIwuiemogkioKaiCSHFrSLSJI4UMJbD2k7bxGJLz/beUdJZvxtM3vHzLaY2dgoTVNQE5GYwmVSUY4sIiYzXgicCzwStXUafopIPA6en+fUmpIZA5hZYzLjOU1VuX8enotcoYKaiMQXfUVBPpMZR6KgJiLxRZ/9zEsy4zgU1EQkHvd8zX5GSmYclyYKRCS+dkpmvD0U1EQkJsdTqUhH1lLctwCNyYznAk80JjM2s1MBzOwwM1sEnAHcbWYf5mqdhp8iEk8etx6KkMz4LYJhaWQKaiISn7YeEpGkcMC1SaSIJIZrk0gRSZhckwDFZF5CW4iY2TJgQbHbUQA1wPJiN0JiSepvNsTd+7alADN7ieDvJ4rl7j6mLfXFVVJBLanMbGaWp6qlBOk3K196Tk1EEkVBTUQSRUGtfUzMfYmUGP1mZUr31EQkUdRTE5FEUVATkURRUCugXEklpPSY2f1m9rWZfVDstsj2UVArkIhJJaT0PAi068Oikl8KaoXTlFTC3euBxqQSUsLc/c/AymK3Q7afglrhtJZUYlCR2iLSYSioFU5BkkqISHYKaoVTkKQSIpKdglrhFCSphIhkp6BWINtKKlHcVkkuZvYoMA0YbmaLzOy8YrdJ4tEyKRFJFPXURCRRFNREJFEU1EQkURTURCRRFNREJFEU1MqImaXM7D0z+8DMnjSzbm0oa6SZPR++PjXbLiJmtoOZXbwddfzKzH4R9fMW1zxoZmNj1LWbdtYQUFArNxvdfYS77wfUAxdmnrRA7N/U3Z9z91uzXLIDEDuoiRSDglr5eh3YM+yhzDWzPwDvALuY2Sgzm2Zm74Q9uh7QtL/bR2Y2Ffh+Y0Fmdq6Z3Rm+3tnMnjGzWeFxNHArsEfYS/xNeN2VZvaWmc02sxszyrou3EPuv4Hhub6EmV0QljPLzP7Yovd5kpm9bma1Zvbd8PpKM/tNRt1/19a/SEkWBbUyZGZVBPu0vR9+NBx42N0PAuqA64GT3P1gYCZwhZl1Ae4B/gr4FtB/G8XfDvyPux8IHAx8CFwNfBL2Eq80s1HAMILtlUYAh5jZt83sEILlYAcRBM3DInydp939sLC+uUDmE/y7AccBfwncFX6H84A17n5YWP4FZjY0Qj3SQVQVuwESS1czey98/TpwHzAQWODu08PPjyTYlPINMwPoRLDsZ2/gM3efD2Bm/w6Ma6WOE4CfALh7ClhjZju2uGZUeLwbvu9BEOR6As+4+4awjihrXfczs18TDHF7ECwra/SEu6eB+Wb2afgdRgEHZNxv6x3WXRuhLukAFNTKy0Z3H5H5QRi46jI/Al5x97NaXDeC/G19ZMAt7n53izp+vh11PAic7u6zzOxcYGTGuZZleVj3Ze6eGfwws91i1isJpeFn8kwHjjGzPQHMrJuZ7QV8BAw1sz3C687axp+fDFwU/tlKM+sFrCPohTWaBPxtxr26QWbWD/gz8D0z62pmPQmGurn0BJaYWTVwTotzZ5hZRdjm3YF5Yd0XhddjZnuZWfcI9UgHoZ5awrj7srDH86iZdQ4/vt7da81sHPBfZrYcmArs10oRfw9MDHenSAEXufs0M3sjfGTixfC+2j7AtLCnuB74sbu/Y2aPA+8BCwiGyLn8EngzvP59mgfPecD/ADsDF7r7JjO7l+Be2zsWVL4MOD3a3450BNqlQ0QSRcNPEUkUBTURSRQFNRFJFAU1EUkUBTURSRQFNRFJFAU1EUmU/w9UO7Pion40EgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = plot_confusion_matrix(clf, X_test, y_test, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive speech'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_to_name(class_label):\n",
    "    \"\"\"\n",
    "    This function is used to map a numeric\n",
    "    feature name to a particular class.\n",
    "    \"\"\"\n",
    "    if class_label == 0:\n",
    "        return \"Offensive speech\"\n",
    "    elif class_label == 1:\n",
    "        return \"Not offensive speech\"\n",
    "    else:\n",
    "        return \"No label\"\n",
    "\n",
    "class_to_name(pipeline.predict(['Being gay is not a crime'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
